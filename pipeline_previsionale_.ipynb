{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43f9cad-1eb5-43c7-ac2d-f012ca2e4187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.13 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.4.0 which is incompatible.\n",
      "google-cloud-pipeline-components 1.0.15 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d1b7cb-d717-4019-a3a7-04569c44a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.1.2)\n",
      "Requirement already satisfied: pickle-mixin in ./.local/lib/python3.7/site-packages (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --user pip pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5fab91-5034-406c-93c1-4460e9b44543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3df0c6-62d3-4ae7-b0b3-9a6b947ebdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.13\n",
      "google_cloud_pipeline_components version: 1.0.15\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e05b5b7-62d7-451e-b502-7fe55291f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"***\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c07cb86-e349-4424-ad3e-fa94a64289c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"europe-west6-a\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc11f706-6cfb-4cb0-adfa-99d146d56243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8771c84-87a1-46b0-8268-2ed68c755b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee943a02-b922-4e67-9191-9b3249d497ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bucket_pipeline_previsionale\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82b5b15-23dc-48fb-b9b6-ffd6615e38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9b7890-b054-4896-9d19-cc3e05f69504",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf73219-f857-44ad-94aa-e2856d1918c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 896262813704-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf20fc49-81f8-49cb-b887-17291e08f373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://bucket_pipeline_previsionale/\n",
      "No changes made to gs://bucket_pipeline_previsionale/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce4ab08a-ed01-4fdd-ba39-2db22fc6969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2842efe-f907-4659-b36a-9a268c8a93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/bucket_pipeline_previsionale\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "311600da-fc7e-427a-a7e2-bf0cb2a8a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca8a78a-417d-41bd-b089-47aee14f730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19281f22-7359-4db3-8ec0-4dbdf547e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\"pandas\",\"db-dtypes\",\"pickle-mixin\",\"numpy\",\"sklearn\"],\n",
    "    base_image=\"python:3.9\"\n",
    ")\n",
    "def preprocess(\n",
    "    message: str,\n",
    "    output_dataset: Output[Dataset],\n",
    "    scaler_out: Output[Model]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from google.cloud import bigquery\n",
    "    import db_dtypes\n",
    "    import pickle\n",
    "    \n",
    "    output_dataset.metadata[\"Message\"] = message + \" preprocessed dataset\"\n",
    "    \n",
    "    client = bigquery.Client.from_service_account_info({\n",
    "      \"type\": \"service_account\",\n",
    "      \"project_id\": \"***\",\n",
    "      \"private_key_id\": \"***\",\n",
    "      \"private_key\": \"***\",\n",
    "      \"client_email\": \"***\",\n",
    "      \"client_id\": \"***\",\n",
    "      \"auth_uri\": \"***\",\n",
    "      \"token_uri\":\"***\",\n",
    "      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "      \"client_x509_cert_url\": \"***\"\n",
    "    })\n",
    "    \n",
    "    df = client.list_rows(\"Acqua_Farina.mezze_giornate_base\").to_dataframe(create_bqstorage_client=True)\n",
    "    client.close()\n",
    "    df[\"Pranzo_Cena\"] = df[\"Pranzo_Cena\"].apply(lambda x: \"A_Pranzo\" if x == \"Pranzo\" else x)\n",
    "    df.sort_values(by=[\"Data\",\"Pranzo_Cena\"],inplace=True)\n",
    "    df.drop(columns=['Importo_netto', 'IVA','ARTICOLO_DA_VENDITA', 'BANCHETTI', 'BIMBI','CENTRIFUGATI', 'COCKTAIL', 'COLAZIONI', 'COMBO',\n",
    "       'CONDPRIMIBABY','CUCINA', 'DESSERT', 'FORMAGGI', 'FORMATI','IMPASTPIZZA','INSEGNE', 'MESDI_',\n",
    "       'MESDI_KARNE','PRODOTTIVEN', 'PROMO',\n",
    "       'SECONDI', 'SMARTBOX', 'Ospiti','Fascia_oraria'],inplace=True)\n",
    "    df.set_index([\"Data\",\"Pranzo_Cena\"],inplace=True)\n",
    "    \n",
    "    n_lag_predizione = 14\n",
    "    n_input = 90\n",
    "    df = df.iloc[-150:]#408\n",
    "    df_train = df[:][:-n_lag_predizione]\n",
    "    df_test = df[:][-n_lag_predizione:]\n",
    "\n",
    "    n_features = len(df.columns)\n",
    "    train = df_train.to_numpy()\n",
    "    train = train.reshape(-1, n_features)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train)\n",
    "    scaled_train = scaler.transform(train)\n",
    "\n",
    "    df.to_csv(output_dataset.path+\".csv\")\n",
    "\n",
    "    with open(scaler_out.path+'.pkl','wb') as f:\n",
    "        pickle.dump(scaler,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfd53ba0-0d30-41d2-a87d-5ec27e5c16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\"pandas\",\"db-dtypes\",\"pickle-mixin\",\"tensorflow\",\"keras\",\"numpy\",\"sklearn\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def make_prediction(\n",
    "    message: str,\n",
    "    df_in: Input[Dataset],\n",
    "    scaler_in: Input[Model],\n",
    "):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from google.cloud import bigquery\n",
    "    import db_dtypes\n",
    "    import pickle\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Reshape\n",
    "    from keras.layers import Dropout\n",
    "    \n",
    "    df = pd.read_csv(df_in.path+\".csv\")\n",
    "    \n",
    "    with open(scaler_in.path+'.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    n_lag_predizione = 14\n",
    "    n_input = 90 \n",
    "    n_features = len(df.columns)\n",
    "    \n",
    "    generator = TimeseriesGenerator(df, df, length=n_input, batch_size=128)\n",
    "    \n",
    "    # Create a function to implement a ModelCheckpoint callback with a specific filename \n",
    "    def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "      return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n",
    "                                            verbose=0, # only output a limited amount of text\n",
    "                                            save_best_only=True,monitor='acc',mode='max',) # save only the best model to file\n",
    "    \n",
    "    model = Sequential(name=\"model_1_dense_mse_adam\")\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(n_features))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0015), loss=\"mse\",metrics=['acc'])\n",
    "    hystory = model.fit(generator, epochs=1000,callbacks=[create_model_checkpoint(model_name=model.name)]) #generator \n",
    "    model = tf.keras.models.load_model(\"model_experiments/\"+model.name)\n",
    "    \n",
    "    test_predictions = []\n",
    "\n",
    "    first_eval_batch = df[-n_input:]\n",
    "    current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "    for i in range(n_lag_predizione):\n",
    "        current_pred = model.predict(current_batch)[0]\n",
    "        test_predictions.append(current_pred) \n",
    "        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) #[[current_pred]]\n",
    "    \n",
    "    true_predictions = scaler.inverse_transform(test_predictions)\n",
    "    df_prediction = pd.DataFrame(data=true_predictions, index = df.index, columns=df.columns)\n",
    "    df_prediction = df_prediction.apply(lambda x: round(x))\n",
    "    df_prediction.reset_index(inplace=True)\n",
    "    \n",
    "    \n",
    "    client2 = bigquery.Client.from_service_account_info({\n",
    "      \"type\": \"service_account\",\n",
    "      \"project_id\": \"***\",\n",
    "      \"private_key_id\": \"***\",\n",
    "      \"private_key\": \"***\",\n",
    "      \"client_email\":\"***\",\n",
    "      \"client_id\": \"***\",\n",
    "      \"auth_uri\": \"***\",\n",
    "      \"token_uri\": \"***\",\n",
    "      \"auth_provider_x509_cert_url\": \"***\",\n",
    "      \"client_x509_cert_url\": \"***\"\n",
    "    })\n",
    "    \n",
    "    job = client2.load_table_from_dataframe(\n",
    "        df_prediction, \"Acqua_Farina.previsioni_test\", job_config=None\n",
    "    )  # Make an API request.\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    client2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81022aa8-8a13-4dbc-bc66-1140e92825c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=\"test-pipeline\",\n",
    ")\n",
    "def pipeline(message: str):\n",
    "    preprocess_task = preprocess(\"INIZIO PREPROCESSING\")\n",
    "    prediction_task = make_prediction(\"FORECASTING\", \n",
    "                    df_in=preprocess_task.outputs[\"output_dataset\"], \n",
    "                    scaler_in = preprocess_task.outputs[\"scaler_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44181ce6-a1d0-4fe9-99a7-9d08c3a6b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"pipeline_previsionale.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b4b2f9-17df-4f38-8504-8c9470b38d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"test_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"pipeline_previsionale.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"message\": \"INIZIO\"},\n",
    ")\n",
    "\n",
    "job.run()\n",
    "\n",
    "! rm pipeline_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2201e2-d363-41e2-9959-2e411df36bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482a192-7f9f-42e4-b92f-4ac1eab9547f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681e82b-d44b-4ca9-a1a3-a938ec12c85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b82e2-368d-4045-8f45-95daad1d4d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed524a6-ec4c-4f91-b2e6-0586f620aee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5c3da-5eb9-4e76-aa43-848f9d3d04c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813ee77-6679-445c-8f86-00b60913b3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6084cf-0490-42f9-8150-c0ff853df46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8927236-afee-4a68-9c47-042f40ad7f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0bd71-0245-4d15-8938-cee793eecda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99726f9-4d7e-49f5-8417-7a24628a050c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b5342-108e-45a3-9563-a731f75ced08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf25807-34b7-4fc9-9e39-46a4f30640ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917efe5-7bc3-4bc3-aef5-2f5da5822fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973682cf-7eef-41b3-b272-990396bf5ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54218f48-fde8-4773-85ae-cb88bd1e97ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
